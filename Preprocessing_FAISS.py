# -*- coding: utf-8 -*-
"""Копия блокнота "Копия блокнота "DL_final_project.ipynb""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1453tda_P_L9SYKI_fvcn77HVGucKt6uX
"""

! pip install python-docx

import docx
import os
import re
import shutil
import random
from docx import Document

def create_directory(path):
    if not os.path.exists(path):
        os.makedirs(path)

def save_article(base_path, doc_name, article_number, content):
    create_directory(base_path)
    file_name = f"{doc_name}_Статья_{article_number.replace('.', '_')}.txt"
    file_path = os.path.join(base_path, file_name)
    with open(file_path, "w", encoding="utf-8") as file:
        file.write(content)

def parse_document(doc, doc_name):
    base_path = os.path.join(os.getcwd(), doc_name)
    article_content = []
    article_number = ""

    for paragraph in doc.paragraphs:
        line = paragraph.text.strip()

        if not line:
            continue

        article_match = re.match(r'Статья\s+(\d+(\.\d+)*)\.\s*(.*)', line)

        if article_match:
            if article_content:
                save_article(base_path, doc_name, article_number, '\n'.join(article_content))
                article_content = []
            article_number = article_match.group(1).strip()
            article_title = article_match.group(3).strip()

        article_content.append(line)

    if article_content:
        save_article(base_path, doc_name, article_number, '\n'.join(article_content))

def read_docx(file_path):
    return Document(file_path)

def main():
    documents = {
        "Жилищный кодекс": ("/content/Living_K.docx"),
        "Земельный кодекс": ("/content/Land_K.docx"),
        "Налоговый кодекс": ("/content/Tax_K.docx"),
        "Трудовой кодекс": ("/content/Working_K.docx"),
        "Гражданский кодекс": ("/content/GK.docx"),
        "Гражданский процессуальный кодекс": ("/content/GKP.docx")
    }

    for doc_name, file_path in documents.items():
        doc = read_docx(file_path)
        parse_document(doc, doc_name)

if __name__ == "__main__":
    main()

def delete_directories(directories):
    for directory in directories:
        directory_path = os.path.join(os.getcwd(), directory)
        if os.path.exists(directory_path):
            shutil.rmtree(directory_path)
            print(f"Папка {directory_path} удалена.")
        else:
            print(f"Папка {directory_path} не существует.")

def main():
    directories = [
        "Жилищный кодекс",
        "Земельный кодекс",
        "Налоговый кодекс",
        "Трудовой кодекс"
    ]

    delete_directories(directories)

if __name__ == "__main__":
    main()

# Чекер количества статей
def count_files_in_directory(directory_path):
    if not os.path.exists(directory_path):
        print(f"Directory '{directory_path}' does not exist.")
        return 0

    file_count = len([name for name in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, name))])
    return file_count

directory_path = "/content/parsed_articles"
file_count = count_files_in_directory(directory_path)

print(f"Количество файлов в папке '{directory_path}': {file_count}")

folder_path = '/content/parsed_dir'

# Удаляем папку со всем содержимым
if os.path.exists(folder_path):
    shutil.rmtree(folder_path)

print("Папка очищена.")

import shutil
shutil.make_archive('/content/parsed_articles', 'zip', '/content/parsed_articles')

def get_random_files(directory_path, num_files=5):
    if not os.path.exists(directory_path):
        print(f"Directory '{directory_path}' does not exist.")
        return []

    all_files = [name for name in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, name))]
    return random.sample(all_files, min(num_files, len(all_files)))

def read_file_content(directory_path, filename):
    filepath = os.path.join(directory_path, filename)
    with open(filepath, "r", encoding="utf-8") as file:
        return file.read()

# directory_path = "/content/Земельный кодекс"
directoru_path = "/content/parsed_dir/fz_for_faiss"
random_files = get_random_files(directory_path)

print(f"Выбрано {len(random_files)} случайных статей из папки '{directory_path}':")
for filename in random_files:
    content = read_file_content(directory_path, filename)
    print(f"\nНазвание файла: {filename}\nСодержимое:\n{content}\n{'-'*80}")

import json


def parse_hierarchy(file_path):
    doc = docx.Document(file_path)
    hierarchy = {}

    # Регулярные выражения для поиска нужных заголовков
    part_regex = re.compile(r'Часть\s+(\w+)')
    section_regex = re.compile(r'Раздел\s+([IVXLCDM]+)\.\s*(.*)')
    subsection_regex = re.compile(r'Подраздел\s+(\d+)\.\s*(.*)')
    chapter_regex = re.compile(r'Глава\s+(\d+)\.\s*(.*)')
    paragraph_regex = re.compile(r'§\s+(\d+(\.\d+)?)\.\s*(.*)')

    current_part = None
    current_section = None
    current_subsection = None
    current_chapter = None
    current_paragraph = None

    for para in doc.paragraphs:
        text = para.text.strip()

        if not text:
            continue

        part_match = part_regex.match(text)
        section_match = section_regex.match(text)
        subsection_match = subsection_regex.match(text)
        chapter_match = chapter_regex.match(text)
        paragraph_match = paragraph_regex.match(text)

        if part_match:
            current_part = f"Часть {part_match.group(1)}"
            if current_part not in hierarchy:
                hierarchy[current_part] = {}
            current_section = None
            current_subsection = None
            current_chapter = None
            current_paragraph = None
        elif section_match:
            current_section = f"Раздел {section_match.group(1)} {section_match.group(2)}"
            if current_part not in hierarchy:
                hierarchy[current_part] = {}
            if current_section not in hierarchy[current_part]:
                hierarchy[current_part][current_section] = {}
            current_subsection = None
            current_chapter = None
            current_paragraph = None
        elif subsection_match:
            current_subsection = f"Подраздел {subsection_match.group(1)} {subsection_match.group(2)}"
            if current_part not in hierarchy:
                hierarchy[current_part] = {}
            if current_section not in hierarchy[current_part]:
                hierarchy[current_part][current_section] = {}
            if current_subsection not in hierarchy[current_part][current_section]:
                hierarchy[current_part][current_section][current_subsection] = {}
            current_chapter = None
            current_paragraph = None
        elif chapter_match:
            current_chapter = f"Глава {chapter_match.group(1)} {chapter_match.group(2)}"
            if current_part not in hierarchy:
                hierarchy[current_part] = {}
            if current_section not in hierarchy[current_part]:
                hierarchy[current_part][current_section] = {}
            if current_subsection not in hierarchy[current_part][current_section]:
                hierarchy[current_part][current_section][current_subsection] = {}
            if current_chapter not in hierarchy[current_part][current_section][current_subsection]:
                hierarchy[current_part][current_section][current_subsection][current_chapter] = {}
            current_paragraph = None
        elif paragraph_match:
            current_paragraph = f"§ {paragraph_match.group(1)} {paragraph_match.group(3)}"
            if current_part not in hierarchy:
                hierarchy[current_part] = {}
            if current_section not in hierarchy[current_part]:
                hierarchy[current_part][current_section] = {}
            if current_subsection not in hierarchy[current_part][current_section]:
                hierarchy[current_part][current_section][current_subsection] = {}
            if current_chapter not in hierarchy[current_part][current_section][current_subsection]:
                hierarchy[current_part][current_section][current_subsection][current_chapter] = {}
            if current_paragraph not in hierarchy[current_part][current_section][current_subsection][current_chapter]:
                hierarchy[current_part][current_section][current_subsection][current_chapter][current_paragraph] = []

    return hierarchy

file_path = "/content/GK.docx"
hierarchy = parse_hierarchy(file_path)

hierarchy_json_path = '/content/hierarchy.json'
with open(hierarchy_json_path, 'w', encoding='utf-8') as f:
    json.dump(hierarchy, f, ensure_ascii=False, indent=4)

with open(hierarchy_json_path, 'r', encoding='utf-8') as f:
    hierarchy_data = f.read()

print(hierarchy_data)



"""варка индекса FAISS"""

pip install faiss-cpu

pip install langchain

import os
import faiss
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from langchain.text_splitter import CharacterTextSplitter
import json

import zipfile

# with zipfile.ZipFile("/content/fz_for_faiss.zip","r") as zip_ref:
#     zip_ref.extractall("parsed_dir")

def extract_zip_with_encoding(zip_file_path, extract_to_path, encoding='utf-8'):
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        for member in zip_ref.infolist():
            # Декодируем имя файла
            decoded_name = member.filename.encode('cp437').decode(encoding)
            target_path = os.path.join(extract_to_path, decoded_name)

            # Проверяем, является ли член директорией
            if member.is_dir():
                if not os.path.exists(target_path):
                    os.makedirs(target_path)
            else:
                # Создаем все необходимые каталоги
                if not os.path.exists(os.path.dirname(target_path)):
                    os.makedirs(os.path.dirname(target_path))

                # Извлекаем файл
                with open(target_path, 'wb') as file:
                    file.write(zip_ref.read(member))

# Пример использования
zip_file_path = "/content/fz_for_faiss.zip"
extract_to_path = "parsed_dir"
extract_zip_with_encoding(zip_file_path, extract_to_path)

# загружаем документы
def load_documents(data_dir):
    documents = []
    metadata = []
    for filename in os.listdir(data_dir):
        if filename.endswith('.txt'):
            with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as file:
                documents.append(file.read())
                metadata.append({'title': filename})
    return documents, metadata

documents_living_k, metadata_living_k = load_documents('/content/Жилищный кодекс')
documents_land_k, metadata_land_k = load_documents('/content/Земельный кодекс')
documents_working_k, metadata_working_k = load_documents('/content/Трудовой кодекс')
documents_tax_k, metadata_tax_k = load_documents('/content/Налоговый кодекс')
documents_gk, metadata_gk = load_documents('/content/Гражданский кодекс')
documents_gkp, metadata_gkp = load_documents('/content/Гражданский процессуальный кодекс')
documents_fz, metadata_fz = load_documents('/content/parsed_dir/fz_for_faiss')

documents_gk[4]
metadata_gk[4]

model_name = 'DeepPavlov/rubert-base-cased-sentence'  # Replace with your model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def chunk_text_with_langchain_custom(text, chunk_size=100, chunk_overlap=30):
    splitter = CharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator=' ',
        length_function=lambda x: len(tokenizer.encode(x, add_special_tokens=False))
    )
    chunks = splitter.split_text(text)
    return chunks

def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embeddings

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# Initialize tokenizer and model
model_name = 'DeepPavlov/rubert-base-cased-sentence'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Assuming chunk_text_with_langchain_custom is defined elsewhere and generates text chunks
all_chunks = []
all_metadata = []
documents = [documents_living_k, documents_land_k, documents_working_k, documents_tax_k, documents_gk, documents_gkp, documents_fz]
metadata = [metadata_living_k, metadata_land_k, metadata_working_k, metadata_tax_k, metadata_gk, metadata_gkp, metadata_fz]

for doc in range(len(documents)):
    for doc_index, document in enumerate(documents[doc]):
        chunks = chunk_text_with_langchain_custom(document)
        all_chunks.extend(chunks)
        for _ in chunks:
            all_metadata.append(metadata[doc][doc_index])

# Define a custom dataset
class TextDataset(Dataset):
    def __init__(self, texts):
        self.texts = texts

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx]

# Define a collate function for batching
def collate_fn(batch):
    inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=512)
    return inputs

# Create dataset and dataloader
dataset = TextDataset(all_chunks)
dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, shuffle=False)

# Ensure model is in evaluation mode and move to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.eval()

all_chunks = []
all_metadata = []
documents = [documents_living_k, documents_land_k, documents_working_k, documents_tax_k, documents_gk, documents_gkp, documents_fz]
metadata = [metadata_living_k, metadata_land_k, metadata_working_k, metadata_tax_k, metadata_gk, metadata_gkp, metadata_fz]

for doc in range(len(documents)):
  for doc_index, document in enumerate(documents[doc]):
      chunks = chunk_text_with_langchain_custom(document)
      all_chunks.extend(chunks)
      for _ in chunks:
          all_metadata.append(metadata[doc][doc_index])

all_chunks[0]

len(all_chunks)

import pickle

with open('chunks.pkl', 'wb') as fp:
    pickle.dump(all_chunks, fp)

with open('all_metadata.pkl', 'wb') as fp:
    pickle.dump(all_metadata, fp)

all_embeddings = []

# Iterate over batches with tqdm for progress bar
for batch in tqdm(dataloader):
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)
    batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
    all_embeddings.extend(batch_embeddings)

# Convert the list of embeddings to a numpy array
embeddings = np.array(all_embeddings)

# FAISS индекс
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

index

# Save the index
faiss.write_index(index, 'faiss_index.index')

# Save the metadata
with open('metadata.json', 'w') as f:
    json.dump(all_metadata, f)

print('FAISS index and metadata created and saved')

# Function to generate embeddings for a query
def embed_query(query):
    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embedding

# Function to search the FAISS index and retrieve metadata
def search_faiss(index, query_embedding, metadata, k=5):
    query_embedding = np.array([query_embedding]).astype('float32')
    distances, indices = index.search(query_embedding, k)
    retrieved_metadata = [metadata[idx] for idx in indices[0]]
    return distances, indices, retrieved_metadata

# Function to generate embeddings for a query
def embed_query(query):
    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the same device as the model
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move the result back to CPU
    return embedding

# Function to search the FAISS index and retrieve metadata
def search_faiss(index, query_embedding, metadata, k=5):
    query_embedding = np.array([query_embedding]).astype('float32')
    distances, indices = index.search(query_embedding, k)
    retrieved_metadata = [metadata[idx] for idx in indices[0]]
    return distances, indices, retrieved_metadata

# Example query
query = "Может ли Частное учреждение осуществлять приносящую доходы деятельность?"
query_embedding = embed_query(query)

# You can then use query_embedding with your FAISS index
# Assuming index is your FAISS index and metadata is the corresponding metadata list
# distances, indices, retrieved_metadata = search_faiss(index, query_embedding, all_metadata)

k = 5
distances, indices, retrieved_metadata = search_faiss(index, query_embedding, all_metadata, k)

retrieved_metadata

print(f"Top {k} nearest neighbors:")
for i, (distance, idx, meta) in enumerate(zip(distances[0], indices[0], retrieved_metadata)):
    print(f"{i + 1}: Document Index {idx} with distance {distance} - Metadata: {meta}")

import zipfile
from google.colab import files

# List of files to include in the zip archive
files_to_include = ['all_metadata.pkl', 'chunks.pkl', 'faiss_index.index', 'metadata.json']

# Name of the zip file
zip_filename = 'all_files_backup.zip'

# Create a zip file containing all the specified files
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    for file in files_to_include:
        zipf.write(file)

# Download the zip file
files.download(zip_filename)

import pandas as pd

df = pd.read_csv('/content/legal_two_answers.csv')

df.head()

df.shape

n = 140

df['question'][n]

df['answer_1'][n]

df['answer_2'][n]

